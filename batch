#!/usr/bin/env python2

import argparse
import copy
import datetime
import multiprocessing
import multiprocessing.dummy
import pscheduler
import sys
import time


#
# Gargle the arguments
#

arg_parser = argparse.ArgumentParser(
    usage="%(prog)s [options]",
    epilog=None,
    formatter_class=argparse.RawTextHelpFormatter
    )

arg_parser.add_argument("--debug",
                      help="Enable debugging to standard error",
                      action="store_true",
                      default=False,
                      dest="debug")

arg_parser.add_argument("--dry",
                      help="Do a dry run",
                      action="store_true",
                      default=False,
                      dest="dry")

args = arg_parser.parse_args()


log = pscheduler.Log(verbose=args.debug, debug=args.debug)

# ------------------------------------------------------------------------------


# Utility functions


def die(message):
    """
    Exit with an error
    """
    # TODO: For Py3, use print(message, file=sys.stderr)
    sys.stderr.write(message + "\n")
    exit(1)


def jq_filter(transform, args={}):
    """
    Make a jq filter using a workaround for a memory corruption
    problem in the production code.

    TODO: Fix production.
    """

    script = transform["script"]
    if isinstance(script, list):
        script = "\n".join(script)

    arg_lines = "".join([
        "%s as $%s | " % (pscheduler.json_dump(value), name)
        for name, value in args.items()
    ])

    return pscheduler.JQFilter(arg_lines + script)



def fail(message):
    """
    Construct a failure for run_task
    """
    return {
        "succeeded": False,
        "error": message
    }


def transformed_spec(spec, transform, iteration=0):
    """
    Run a transform against a spec and an iteration
    """

    assert isinstance(spec, dict)

    if transform is None:
        return spec

    # We overwrite this, so don't clobber the original.
    result = copy.deepcopy(spec)

    filterer = jq_filter(transform, args={"iteration": iteration})

    result = filterer(result)[0]

    return result



def run_task(task_args):
    """
    Run a single task and return the result.
    """

    label, delay, spec, log = task_args

    log.debug("%s: Spec is %s", label, pscheduler.json_dump(spec, pretty=True))

    if args.dry:
        log.debug("%s: Dry run; skipping task", label)
        return {}

    if delay:
        log.debug("%s: Sleeping %ss before start", label, delay)
        time.sleep(delay)

    tasks_url = pscheduler.api_url(path="/tasks")
    log.debug("%s: Posting to %s", label, tasks_url)

    try:
        status, task_url = pscheduler.url_post(tasks_url, data=pscheduler.json_dump(spec))
    except Exception as ex:
        log.debug("Failed: %s", str(ex))
        return fail("Failed to post task: %s" % (str(ex)))

    log.debug("%s: New task is %s", label, task_url)


    # Fetch the posted task with extra details.

    try:
        status, task_data = pscheduler.url_get(task_url, params={"detail": True}, throw=False)
        if status != 200:
            return fail("Failed to get task data: %s" % (task_data))
    except Exception as ex:
        return fail("Failed to get task data: %s" % (str(ex)))

    try:
        first_run_url = task_data["detail"]["first-run-href"]
    except KeyError:
        return fail("Server returned incomplete data.")

    log.debug("%s: First run is at %s", label, first_run_url)


    # Get the first run

    status, run_data = pscheduler.url_get(first_run_url, throw=False)

    if status == 404:
        return fail("The server never scheduled a run for the task.")
    if status != 200:
        return fail("Error %d: %s" % (status, run_data))

    for key in ["start-time", "end-time", "result-href"]:
        if key not in run_data:
            return fail("Server did not return %s with run data" % (key))

    log.debug("%s: First run is %s", label, run_data["href"])

    # Wait for the end time to pass.

    try:
        end_time = pscheduler.iso8601_as_datetime(run_data["end-time"])
    except ValueError as ex:
        return fail("Server did not return a valid end time for the task: %s" % (str(ex)))

    now = pscheduler.time_now()
    sleep_time = end_time - now if end_time > now else datetime.timedelta()
    sleep_seconds = sleep_time.total_seconds()

    log.debug("%s: Waiting %s seconds for run to finish", label, sleep_seconds)
    time.sleep(sleep_seconds)

    # Wait for the result to come available and fetch it.

    # TODO: If everything stops all at once, there will be a run on
    # the server.  Use a mutex to serialize this.

    log.debug("%s: Waiting for result at %s", label, run_data["result-href"])

    status, result_data = pscheduler.url_get(run_data["result-href"],
                                             params={"wait-merged": True},
                                             throw=False)

    # TODO: 404 is only semi-kosher becasue bgmulti doesn't generate a
    # result.  Need a fix in the server for this.
    if status not in [200, 404]:
        return fail("Did not get a result: %s" % (result_data))


    # Get all runs and return them in all three formats

    status, runs = pscheduler.url_get(task_data["detail"]["runs-href"], throw=False)
    if status != 200:
        return fail("Error %d getting runs: %s" % (status, runs))

    run_data = []

    for run in runs:

        log.debug("%s: Fetching run %s", label, run)

        status, run_json = pscheduler.url_get(run)
        if status != 200:
            # TODO: Probably not what we want.
            return [ { "application/json": fail("Error %d getting runs: %s" % (status, runs)) } ]

        result_href = run_json["result-href"]

        run_set = { "application/json": run_json }

        if "result-merged" in run_json:
            for text_format in [ "application/json", "text/plain", "text/html" ]:
                is_json = text_format == "application/json"
                status, text = pscheduler.url_get(result_href,
                                                  params={"format": text_format},
                                                  json=is_json,
                                                  throw=False)
                if status == 200:
                    log.debug("%s: Got %s result", label, text_format)
                    run_set[text_format] = text
                else:
                    log.debug("%s: Failed to get %s: %d: %s", label, text_format, status, text)
                    run_set[text_format] = fail("%d: %s" % (status, text))


        run_data.append(run_set)


    return run_data




# ------------------------------------------------------------------------------

# Get and validate the input

try:
    json = pscheduler.json_load(sys.stdin)
except Exception as ex:
    die(str(ex))

INPUT_SCHEMA = {

    "local": {
        "job": {
            "type": "object",
            "properties": {
                "label": { "$ref": "#/pScheduler/String" },
                "enabled": { "$ref": "#/pScheduler/Boolean" },
                "iterations": { "$ref": "#/pScheduler/Cardinal" },
                "parallel": { "$ref": "#/pScheduler/Boolean" },
                "backoff": { "$ref": "#/pScheduler/Duration" },
                "sync-start": { "$ref": "#/pScheduler/Boolean" },
                "task": { "$ref": "#/pScheduler/AnyJSON" },  # Validated later.
                "task-transform": { "$ref": "#/pScheduler/JQTransformSpecification" }
            },
            "additionalProperties": False,
            "required": [
                "task"
            ]
        }
    },

    "type": "object",
    "properties": {
        "schema": { "type": "integer" },
        "jobs": { 
            "type": "array",
            "items": { "$ref": "#/local/job" }
        },
    },
    "additionalProperties": False,
    "required": [
        "jobs",
    ]
}

# Validate the basic structure
valid, message = pscheduler.json_validate(json, INPUT_SCHEMA, max_schema=1)
if not valid:
    die(message)


# Validate the jobs
for job_number, job in enumerate(json["jobs"]):

    if not job.get("enabled", True):
        log.debug("Skipping disabled job %d", job_number)
        continue

    log.debug("Validating %s: %s",
              job.get("label", "job %d" % (job_number)),
              pscheduler.json_dump(job, pretty=True))

    if "transform" in job:
        iterations = job.get("iterations", 1)
    else:
        log.debug("No transform, validating only one iteration.")
        iterations = 1

    for iteration in range(0, iterations):
        log.debug("Iteration %d", iteration)
        try:
            transformed = transformed_spec(job["task"], job.get("task-transform", None), iteration)
            log.debug("Transformed to %s", pscheduler.json_dump(transformed, pretty=True))
        except (ValueError, pscheduler.jqfilter.JQRuntimeError) as ex:
            die("At /jobs/%d/task-transform iteration %d: %s" % (job_number, iteration, ex))

        valid, message = pscheduler.json_validate(transformed, { "$ref": "#/pScheduler/TaskSpecification" })
        if not valid:
            die("At /jobs/%d/task iteration %d: %s" % (job_number, iteration, message))



# ------------------------------------------------------------------------------


for job in json["jobs"]:

    iterations = job.get("iterations", 1)
    label = job.get("label", "unlabeled")

    if iterations == 0 or (not job.get("enabled", True)):
        log.debug("%s: Disabled", label)
        job["results"] = []
        continue

    spec = job["task"]

    transform = job.get("task-transform", None)

    # TODO: It might be better to use a mutex to serialize access to
    # the server instead of backoff.

    backoff = pscheduler.iso8601_as_timedelta(job.get("backoff", "P0D"))
    backoff_secs = pscheduler.timedelta_as_seconds(backoff)

    if "schedule" in spec:

        schedule = spec["schedule"]

        # Scrub out anything that shouldn't be there.
        for parameter in [ "start", "repeat", "repeat-cron", "until", "max-runs" ]:
            try:
                del schedule[parameter]
            except KeyError:
                pass  # Not there?  Don't care.

    else:

        spec["schedule"] = {}


    parallel = job.get("parallel", False)

    if parallel and job.get("sync-start", True):

        # Make everything past all backoffs and setup time.
        start_delay = (backoff * iterations) \
                      + pscheduler.iso8601_as_timedelta(job.get("setup-time", "PT15S"))
        start = pscheduler.datetime_as_iso8601(pscheduler.time_now() + start_delay)

        log.debug("%s: Pushing start out %s to %s", label, start_delay, start)
        spec["schedule"]["start"] = start


    run_task_args = []

    for iteration in range(0,iterations):

        label_num = "%s/%d" % (label, iteration)

        # TODO: This throws if there's a problem.
        transformed = transformed_spec(job["task"], transform, iteration)

        run_task_args.append((label_num,
                              backoff_secs * iteration,
                              transformed,
                              log))


    if parallel:

        log.debug("%s: Parallel run of %d tasks", label, iteration)

        with multiprocessing.dummy.Pool(processes=iteration) as pool:
            job["results"] = list(pool.imap(run_task, run_task_args, chunksize=1))

    else:

        job["results"] = [ run_task(arg) for arg in run_task_args ]



print(pscheduler.json_dump(json, pretty=True))
pscheduler.succeed()
