#!/usr/bin/env python2

import argparse
import copy
import datetime
import multiprocessing
import multiprocessing.dummy
import pscheduler
import sys
import threading
import time


#
# Gargle the arguments
#

arg_parser = argparse.ArgumentParser(
    usage="%(prog)s [options]",
    epilog=None,
    formatter_class=argparse.RawTextHelpFormatter
    )

arg_parser.add_argument("--debug",
                      help="Enable debugging to standard error",
                      action="store_true",
                      default=False,
                      dest="debug")

arg_parser.add_argument("--dry",
                      help="Do a dry run",
                      action="store_true",
                      default=False,
                      dest="dry")

args = arg_parser.parse_args()


log = pscheduler.Log(verbose=args.debug, debug=args.debug)

# ------------------------------------------------------------------------------


# Utility functions


def jq_filter(transform, args={}):
    """
    Make a jq filter using a workaround for a memory corruption
    problem in the production code.

    TODO: Fix production.
    """

    if transform is None:
        script = "."
    else:
        script = transform["script"]

    if isinstance(script, list):
        script = "\n".join(script)

    arg_lines = "".join([
        "%s as $%s | " % (pscheduler.json_dump(value), name)
        for name, value in args.items()
    ])

    return pscheduler.JQFilter(arg_lines + script)



def fail(message):
    """
    Construct a failure for run_task
    """
    return {
        "succeeded": False,
        "error": message
    }


def transformed_spec(spec, transform, iteration=0, globaldata=None):
    """
    Run a transform against a spec and an iteration
    """

    assert isinstance(spec, dict)

    if transform is None:
        return spec

    filterer = jq_filter(transform, args={
        "global": globaldata,
        "iteration": iteration
    })

    return filterer(spec)[0]



def run_task(task_args):
    """
    Run a single task and return the result.
    """

    label, delay, spec = task_args

    log.debug("%s: Spec is %s", label, pscheduler.json_dump(spec, pretty=True))

    if args.dry:
        log.debug("%s: Dry run; skipping task", label)
        return {}

    if delay:
        log.debug("%s: Sleeping %ss before start", label, delay)
        time.sleep(delay)

    tasks_url = pscheduler.api_url(path="/tasks")
    log.debug("%s: Posting to %s", label, tasks_url)

    try:
        status, task_url = pscheduler.url_post(tasks_url, data=pscheduler.json_dump(spec))
    except Exception as ex:
        log.debug("Failed: %s", str(ex))
        return fail("Failed to post task: %s" % (str(ex)))

    log.debug("%s: New task is %s", label, task_url)


    # Fetch the posted task with extra details.

    try:
        status, task_data = pscheduler.url_get(task_url, params={"detail": True}, throw=False)
        if status != 200:
            return fail("Failed to get task data: %s" % (task_data))
    except Exception as ex:
        return fail("Failed to get task data: %s" % (str(ex)))

    try:
        first_run_url = task_data["detail"]["first-run-href"]
    except KeyError:
        return fail("Server returned incomplete data.")

    log.debug("%s: First run is at %s", label, first_run_url)


    # Get the first run

    status, run_data = pscheduler.url_get(first_run_url, throw=False)

    if status == 404:
        return fail("The server never scheduled a run for the task.")
    if status != 200:
        return fail("Error %d: %s" % (status, run_data))

    for key in ["start-time", "end-time", "result-href"]:
        if key not in run_data:
            return fail("Server did not return %s with run data" % (key))

    log.debug("%s: First run is %s", label, run_data["href"])

    # Wait for the end time to pass.

    try:
        end_time = pscheduler.iso8601_as_datetime(run_data["end-time"])
    except ValueError as ex:
        return fail("Server did not return a valid end time for the task: %s" % (str(ex)))

    now = pscheduler.time_now()
    sleep_time = end_time - now if end_time > now else datetime.timedelta()
    sleep_seconds = sleep_time.total_seconds()

    log.debug("%s: Waiting %s seconds for run to finish", label, sleep_seconds)
    time.sleep(sleep_seconds)

    # Wait for the result to come available and fetch it.

    # TODO: If everything stops all at once, there will be a run on
    # the server.  Use a mutex to serialize this.

    log.debug("%s: Waiting for result at %s", label, run_data["result-href"])

    status, result_data = pscheduler.url_get(run_data["result-href"],
                                             params={"wait-merged": True},
                                             throw=False)

    # TODO: 404 is only semi-kosher becasue bgmulti doesn't generate a
    # result.  Need a fix in the server for this.
    if status not in [200, 404]:
        return fail("Did not get a result: %s" % (result_data))


    # Get all runs and return them in all three formats

    status, runs = pscheduler.url_get(task_data["detail"]["runs-href"], throw=False)
    if status != 200:
        return fail("Error %d getting runs: %s" % (status, runs))

    run_data = []

    for run in runs:

        log.debug("%s: Fetching run %s", label, run)

        status, run_json = pscheduler.url_get(run)
        if status != 200:
            # TODO: Probably not what we want.
            return [ { "application/json": fail("Error %d getting runs: %s" % (status, runs)) } ]

        result_href = run_json["result-href"]

        run_set = { "application/json": run_json }

        if "result-merged" in run_json:
            for text_format in [ "application/json", "text/plain", "text/html" ]:
                is_json = text_format == "application/json"
                status, text = pscheduler.url_get(result_href,
                                                  params={"format": text_format},
                                                  json=is_json,
                                                  throw=False)
                if status == 200:
                    log.debug("%s: Got %s result", label, text_format)
                    run_set[text_format] = text
                else:
                    log.debug("%s: Failed to get %s: %d: %s", label, text_format, status, text)
                    run_set[text_format] = fail("%d: %s" % (status, text))


        run_data.append(run_set)


    return {
        "task": spec,
        "runs": run_data
    }




# ------------------------------------------------------------------------------

# Get and validate the input

try:
    json = pscheduler.json_load(sys.stdin)
except Exception as ex:
    pscheduler.fail(str(ex))


INPUT_SCHEMA = {

    "local": {

        "global": {
            "type": "object",
            "properties": {
                "data": { "$ref": "#/pScheduler/AnyJSON" },
                "transform-post": { "$ref": "#/pScheduler/JQTransformSpecification" },
                "transform-pre": { "$ref": "#/pScheduler/JQTransformSpecification" }
            },
            "additionalProperties": False
        },

        "job": {
            "type": "object",
            "properties": {
                "label": { "$ref": "#/pScheduler/String" },
                "enabled": { "$ref": "#/pScheduler/Boolean" },
                "iterations": { "$ref": "#/pScheduler/Cardinal" },
                "parallel": { "$ref": "#/pScheduler/Boolean" },
                "backoff": { "$ref": "#/pScheduler/Duration" },
                "sync-start": { "$ref": "#/pScheduler/Boolean" },
                "task": { "$ref": "#/pScheduler/AnyJSON" },  # Validated later.
                "task-transform": { "$ref": "#/pScheduler/JQTransformSpecification" }
            },
            "additionalProperties": False,
            "required": [
                "task"
            ]
        }
    },

    "type": "object",
    "properties": {
        "schema": { "type": "integer" },
        "global": { "$ref": "#/local/global" },
        "jobs": { 
            "type": "array",
            "items": { "$ref": "#/local/job" }
        },
    },
    "additionalProperties": False,
    "required": [
        "jobs",
    ]
}

# Validate the basic structure
# TODO: Restore max_schema=1 after Python 2 goes away.
valid, message = pscheduler.json_validate(json, INPUT_SCHEMA)
if not valid:
    pscheduler.fail(message)


try:
    global_data = json["global"]["data"]
except KeyError:
    global_data = None

try:
    global_transform_pre = json["global"]["transform-pre"]
except KeyError:
    global_transform_pre = { "script": "." }

try:
    global_transform_post = json["global"]["transform-post"]
except KeyError:
    global_transform_post = { "script": "." }



# Validate and build up the jobs and do the transforms on each

INTERNAL = "_internal"


def apply_transform(data,
                    transform,
                    args,
                    transform_location,
                    runtime_location
):
    """
    Apply a transform and return the results, exiting if compilation
    or runtime fails.
    """
        
    try:
        filterer = jq_filter(transform, args=args)
        return filterer(data)[0]
    except ValueError as ex:
        pscheduler.fail("At %s: %s" % (transform_location, ex))
    except pscheduler.jqfilter.JQRuntimeError as ex:
        pscheduler.fail("At %s: %s" % (runtime_location, ex))

    assert False, "Should not be reached."



for job_number, job in enumerate(json["jobs"]):

    if not job.get("enabled", True):
        log.debug("Skipping disabled job %d", job_number)
        continue


    log.debug("Preparing %s: %s",
              job.get("label", "job %d" % (job_number)),
              pscheduler.json_dump(job, pretty=True))

    iterations = job.get("iterations", 1)

    backoff = pscheduler.iso8601_as_timedelta(job.get("backoff", "P0D"))
    backoff_secs = pscheduler.timedelta_as_seconds(backoff)

    run_task_args = []

    for iteration in range(0, iterations):
        log.debug("Iteration %d", iteration)

        transform_args = {
            "global": global_data,
            "iteration": iteration
        }

        log.debug("Starting with %s", pscheduler.json_dump(job["task"], pretty=True))

        # Global Pre Transform
        global_pre_transformed = apply_transform(job["task"],
                                                 global_transform_pre,
                                                 transform_args,
                                                 "/global/transform-pre",
                                                 "/jobs/%d, iteration %d, global pre-transform" % (job_number, iteration)
                                             )

        # Job Task Transform
        job_task_transformed = apply_transform(global_pre_transformed,
                                               job.get("task-transform"),
                                               transform_args,
                                               "/jobs/%d/task-transform" % (job_number),
                                               "/jobs/%d/task-transform, iteration %d" % (job_number, iteration)
                                           )

        # Global Post Transform
        transformed = apply_transform(job_task_transformed,
                                      global_transform_post,
                                      transform_args,
                                      "/global/transform-post",
                                      "/jobs/%d, iteration %d, global post-transform" % (job_number, iteration)
        )



        # Make sure the task back up structurally-valid

        log.debug("Validating %s", pscheduler.json_dump(transformed, pretty=True))

        # TODO: Get rid of the Py2-specific version
        if (sys.version_info > (3, 0)):
            valid, message = pscheduler.json_validate(transformed,
                                                      { "$ref": "#/pScheduler/TaskSpecification" })
        else:
            valid, message = pscheduler.json_validate({ "transformed": transformed },
                                                      {
                                                          "type": "object",
                                                          "properties": {
                                                              "transformed": { "$ref": "#/pScheduler/TaskSpecification" }
                                                          },
                                                          "additionalProperties": False
                                                      })

        if not valid:
            pscheduler.fail("At /jobs/%d/task iteration %d: %s" % (job_number, iteration, message))

        if "schedule" in transformed:

            schedule = transformed["schedule"]

            # Scrub out anything that shouldn't be there.
            for parameter in [ "start", "repeat", "repeat-cron", "until", "max-runs" ]:
                try:
                    del schedule[parameter]
                except KeyError:
                    pass  # Not there?  Don't care.

        else:

            transformed["schedule"] = {}


        job["task"] = transformed

        # Hold it for run time.
        run_task_args.append((
            "%s/%d" % (job.get("label", "unlabeled"), iteration),
            backoff_secs * iteration,
            transformed
        ))

    job[INTERNAL] = run_task_args



# ------------------------------------------------------------------------------

# Run it.

for job in json["jobs"]:

    label = job.get("label", "unlabeled")

    log.debug("Job %s: %s", label, pscheduler.json_dump(job, pretty=True))

    if not job.get("enabled", True):
        log.debug("%s: Disabled", label)
        job["results"] = []
        continue

    run_task_args = job.get(INTERNAL, [])
    iterations = len(run_task_args)
    internal = job[INTERNAL]

    # TODO: It might be better to use a mutex to serialize access to
    # the server instead of backoff.

    backoff = pscheduler.iso8601_as_timedelta(job.get("backoff", "P0D"))
    backoff_secs = pscheduler.timedelta_as_seconds(backoff)

    parallel = job.get("parallel", False)


    if parallel and job.get("sync-start", True):

        # Make everything past all backoffs and setup time.
        start_delay = (backoff * iterations) \
                      + pscheduler.iso8601_as_timedelta(job.get("setup-time", "PT15S"))
        start = pscheduler.datetime_as_iso8601(pscheduler.time_now() + start_delay)

        log.debug("%s: Pushing start out %s to %s", label, start_delay, start)

        # Inject the schedule into all runs.
        for run in internal:
            label, backoff, spec = run
            spec["schedule"]["start"] = start


    if parallel:

        log.debug("%s: Parallel run of %d tasks", label, iterations)

        # TODO: Get rid of the Py2-specific version
        if (sys.version_info > (3, 0)):
            with multiprocessing.dummy.Pool(processes=iterations) as pool:
                job["results"] = list(pool.imap(run_task, run_task_args, chunksize=1))
        else:
            try:
                pool = multiprocessing.dummy.Pool(processes=iterations)
                job["results"] = list(pool.imap(run_task, run_task_args, chunksize=1))
            finally:
                pool.close()

            

    else:

        job["results"] = [ run_task(arg) for arg in run_task_args ]



# Remove any leftover internal bits before printing the final result.

for job in json["jobs"]:
    try:
        del job[INTERNAL]
    except KeyError:
        pass

# TODO: Switch to succeed_json
print(pscheduler.json_dump(json, pretty=True))
pscheduler.succeed()
